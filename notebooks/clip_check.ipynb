{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L3HMvdfKl6EI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14fe9cb9-bbc2-41c7-f3af-00704857a60e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===> Starting Visual_WSD Downloader\n",
            "Visual_WSD dataset downloaded\n",
            "Visual_WSD dataset unzipped\n",
            "visual_wsd zip file removed\n",
            "Visual_WSD dataset folders renamed\n",
            "\n",
            "===> Starting Visual_WSD Restructurer\n",
            "Visual_WSD dataset trial images reanamed\n",
            "Visual_WSD dataset txt files parsed to csv\n",
            "Visual_WSD dataset restructured\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "This module downloads Visual-WSD dataset and renames/restructures/simplifies it.\n",
        "All trial images are renamed (so they don't cross with train images) and moved to train ones.\n",
        "All txt files are parsed and combined to single csv.\n",
        "\n",
        "Final simplified structure of dataset is:\n",
        "    -data\n",
        "        -visual_wsd\n",
        "            -images\n",
        "                -image.number.jpg\n",
        "                ...\n",
        "            -dataset.csv\n",
        "\n",
        "Columns of csv are: ['ambigues word', 'context (phrase)', 'target_image', {image_1 - image_9}(wrong images)]\n",
        "\"\"\"\n",
        "\n",
        "import asyncio\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "from typing import Literal\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import aiohttp\n",
        "import pandas as pd\n",
        "from aiohttp import ClientResponse, ClientTimeout\n",
        "\n",
        "###### UNCOMMENT THIS IF YOU RUN IN JUPYTER ENVIROMENT\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "\n",
        "class VisualWSDDownloader:\n",
        "    \"\"\"\n",
        "    This class handles the downloading of the Visual-WSD dataset from Google Drive.\n",
        "    It manages the virus scan page for large files, downloads the dataset in zip format, unzips it, and cleans up by removing the zip file.\n",
        "    Additionally, it provides functionality to rename the dataset directories to a more manageable format.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, file_gdrive_id: str, zip_file_path: str, extract_to_path: str\n",
        "    ) -> None:\n",
        "        self.file_gdrive_id = file_gdrive_id\n",
        "        self.zip_file_path = zip_file_path\n",
        "        self.extract_to_path = extract_to_path\n",
        "\n",
        "    async def download_file_from_google_drive(self) -> None:\n",
        "        URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "        CUSTOM_TIMEOUT = 1000\n",
        "        timeout = ClientTimeout(total=CUSTOM_TIMEOUT)\n",
        "\n",
        "        async with aiohttp.ClientSession(timeout=timeout) as session:\n",
        "            initial_response = await session.get(\n",
        "                URL, params={\"id\": self.file_gdrive_id}\n",
        "            )\n",
        "            token = await self.get_confirm_token(initial_response)\n",
        "\n",
        "            if token:\n",
        "                params = {\"id\": self.file_gdrive_id, \"confirm\": token}\n",
        "                response = await session.get(URL, params=params)\n",
        "            else:\n",
        "                response = initial_response\n",
        "\n",
        "            await self.save_response_content(response)\n",
        "\n",
        "    async def get_confirm_token(self, response: ClientResponse) -> str:\n",
        "        if \"text/html\" in response.headers.get(\"Content-Type\", \"\"):\n",
        "            text = await response.text()\n",
        "            match = re.search(\"confirm=([0-9A-Za-z_]+)&\", text)\n",
        "            return match.group(1) if match else None\n",
        "        return None\n",
        "\n",
        "    async def save_response_content(self, response: ClientResponse) -> None:\n",
        "        CHUNK_SIZE = 32768\n",
        "        with open(self.zip_file_path, \"wb\") as f:\n",
        "            async for chunk in response.content.iter_chunked(CHUNK_SIZE):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "\n",
        "    def unzip_file(self) -> None:\n",
        "        with ZipFile(self.zip_file_path, \"r\") as zip_ref:\n",
        "            zip_ref.extractall(self.extract_to_path)\n",
        "\n",
        "    def rename_directories(self) -> None:\n",
        "        os.chdir(\"./data/\")\n",
        "        os.rename(\"./semeval-2023-task-1-V-WSD-train-v1\", \"./visual_wsd\")\n",
        "        os.rename(\"./visual_wsd/train_v1\", \"./visual_wsd/train\")\n",
        "        os.rename(\"./visual_wsd/trial_v1\", \"./visual_wsd/trial\")\n",
        "        os.rename(\"./visual_wsd/train/train_images_v1\", \"./visual_wsd/train/images\")\n",
        "        os.rename(\"./visual_wsd/trial/trial_images_v1\", \"./visual_wsd/trial/images\")\n",
        "        os.chdir(\"../\")\n",
        "        if os.path.exists(self.zip_file_path):\n",
        "            print(\"visual_wsd zip file removed\")\n",
        "            os.remove(self.zip_file_path)\n",
        "\n",
        "    async def run(self) -> None:\n",
        "        print(\"===> Starting Visual_WSD Downloader\")\n",
        "        await self.download_file_from_google_drive()\n",
        "        print(\"Visual_WSD dataset downloaded\")\n",
        "        self.unzip_file()\n",
        "        print(\"Visual_WSD dataset unzipped\")\n",
        "        self.rename_directories()\n",
        "        print(\"Visual_WSD dataset folders renamed\\n\")\n",
        "\n",
        "\n",
        "class VisualWSDRestructurer:\n",
        "    \"\"\"\n",
        "    This class is responsible for reorganizing the Visual-WSD dataset.\n",
        "    It renames and moves trial images to avoid name conflicts with training images, parses text files related to the dataset,\n",
        "    and combines this information into a single CSV file. The class also restructures the dataset into a simplified format\n",
        "    with a specific folder structure and dataset CSV.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_path: str, dataset_name: str) -> None:\n",
        "        self.data_path = data_path\n",
        "        self.dataset_name = dataset_name\n",
        "        self.path = os.path.join(data_path, dataset_name)\n",
        "\n",
        "        self.max_num = self.find_max_image_number(\n",
        "            os.path.join(self.path, \"train\", \"images\")\n",
        "        )\n",
        "\n",
        "    def find_max_image_number(self, images_path: str) -> int:\n",
        "        max_num = 0\n",
        "        for image_file in os.listdir(images_path):\n",
        "            num = int(re.search(r\"\\d+\", image_file).group())\n",
        "            if num > max_num:\n",
        "                max_num = num\n",
        "        return max_num + 1\n",
        "\n",
        "    def rename_move_trial_images(self) -> int:\n",
        "        trial_images_path = os.path.join(self.path, \"trial\", \"images\")\n",
        "        train_images_path = os.path.join(self.path, \"train\", \"images\")\n",
        "        for filename in os.listdir(trial_images_path):\n",
        "            match = re.search(r\"\\d+\", filename)\n",
        "            if match:\n",
        "                number = int(match.group())\n",
        "                new_number = self.max_num + number\n",
        "                new_filename = filename.replace(str(number), str(new_number))\n",
        "                shutil.move(\n",
        "                    os.path.join(trial_images_path, filename),\n",
        "                    os.path.join(train_images_path, new_filename),\n",
        "                )\n",
        "\n",
        "        if not os.listdir(trial_images_path):\n",
        "            shutil.rmtree(trial_images_path)\n",
        "\n",
        "    def txt2csv(\n",
        "        self, path: str, datafile: str, goldfile: str, mode: Literal[\"train\", \"trial\"]\n",
        "    ) -> None:\n",
        "        column_names = [\"word\", \"context\", \"target\"] + [\n",
        "            f\"image_{i}\" for i in range(1, 11)\n",
        "        ]\n",
        "\n",
        "        data_file_path = os.path.join(path, datafile)\n",
        "        df1 = pd.read_csv(data_file_path, sep=\"\\t\", header=None)\n",
        "\n",
        "        gold_file_path = os.path.join(path, goldfile)\n",
        "        df2 = pd.read_csv(gold_file_path, sep=\"\\t\", header=None)\n",
        "\n",
        "        combined_df = pd.concat([df1.iloc[:, :2], df2, df1.iloc[:, 2:12]], axis=1)\n",
        "        combined_df.columns = column_names\n",
        "\n",
        "        def update_image_name(image_name: str) -> None:\n",
        "            if mode == \"trial\":\n",
        "                num = int(image_name.split(\".\")[1]) + self.max_num\n",
        "                return f\"image.{num}.jpg\"\n",
        "            return image_name\n",
        "\n",
        "        combined_df[\"target\"] = combined_df[\"target\"].apply(update_image_name)\n",
        "        for i in range(1, 11):\n",
        "            combined_df[f\"image_{i}\"] = combined_df[f\"image_{i}\"].apply(\n",
        "                update_image_name\n",
        "            )\n",
        "\n",
        "        combined_df[\"images\"] = combined_df[\n",
        "            [f\"image_{i}\" for i in range(1, 11)]\n",
        "        ].values.tolist()\n",
        "        combined_df[\"images\"] = combined_df.apply(\n",
        "            lambda row: [img for img in row[\"images\"] if img != row[\"target\"]], axis=1\n",
        "        )\n",
        "\n",
        "        for i in range(1, 10):\n",
        "            combined_df[f\"image_{i}\"] = combined_df[\"images\"].apply(\n",
        "                lambda x: x[i - 1] if i <= len(x) else None\n",
        "            )\n",
        "        combined_df.drop(columns=[\"images\", \"image_10\"], inplace=True)\n",
        "\n",
        "        combined_df.to_csv(os.path.join(path, \"dataset.csv\"), index=False)\n",
        "\n",
        "    def restructure(self) -> None:\n",
        "        shutil.move(\n",
        "            os.path.join(self.path, \"train\", \"images\"),\n",
        "            os.path.join(self.path, \"images\"),\n",
        "        )\n",
        "        shutil.move(\n",
        "            os.path.join(self.path, \"train\", \"dataset.csv\"),\n",
        "            os.path.join(self.path, \"dataset.csv\"),\n",
        "        )\n",
        "        shutil.rmtree(os.path.join(self.path, \"train\"))\n",
        "        shutil.rmtree(os.path.join(self.path, \"trial\"))\n",
        "\n",
        "    def run(self) -> None:\n",
        "        print(\"===> Starting Visual_WSD Restructurer\")\n",
        "        self.rename_move_trial_images()\n",
        "        print(\"Visual_WSD dataset trial images reanamed\")\n",
        "        self.txt2csv(\n",
        "            path=os.path.join(self.path, \"train\"),\n",
        "            datafile=\"train.data.v1.txt\",\n",
        "            goldfile=\"train.gold.v1.txt\",\n",
        "            mode=\"train\",\n",
        "        )\n",
        "        print(\"Visual_WSD dataset txt files parsed to csv\")\n",
        "        self.restructure()\n",
        "        print(\"Visual_WSD dataset restructured\\n\")\n",
        "\n",
        "\n",
        "async def main():\n",
        "    visual_wsd_downloader = VisualWSDDownloader(\n",
        "        \"1byX4wpe1UjyCVyYrT04sW17NnycKAK7N\", \"./visual_wsd.zip\", \"./data/\"\n",
        "    )\n",
        "    await visual_wsd_downloader.run()\n",
        "    visual_wsd_restructurer = VisualWSDRestructurer(\"./data\", \"visual_wsd\")\n",
        "    visual_wsd_restructurer.run()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    loop = asyncio.get_event_loop()\n",
        "    loop.run_until_complete(main())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Literal, Optional\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import Compose\n",
        "\n",
        "PIL.Image.MAX_IMAGE_PIXELS = 1000000000\n",
        "\n",
        "class VisualWSDDataset(Dataset):\n",
        "    \"\"\"\n",
        "    This class implements a torch dataset for the Visual-WSD dataset, inheriting from PyTorch's Dataset class.\n",
        "    The class supports both training and evaluation modes and includes functionality for splitting the dataset\n",
        "    into training and evaluation subsets. It also supports custom transformations on the images.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        path: str,\n",
        "        csv_file: str,\n",
        "        images_folder: str,\n",
        "        transform: Optional[Compose] = None,\n",
        "        mode: Literal[\"train\", \"eval\"] = \"eval\",\n",
        "        train_ratio: float = 0.8,\n",
        "    ) -> None:\n",
        "        self.path = path\n",
        "        self.df = pd.read_csv(os.path.join(path, csv_file))\n",
        "        self.images_folder = images_folder\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "        self.train_ratio = train_ratio\n",
        "\n",
        "        if mode == \"train\":\n",
        "            self.train_data, self.test_data = train_test_split(\n",
        "                self.df, train_size=train_ratio\n",
        "            )\n",
        "        elif mode == \"eval\":\n",
        "            self.data = self.df\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Invalid mode. Choose 'train' or 'eval'. Provided mode: {mode}\"\n",
        "            )\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        if self.mode == \"train\":\n",
        "            return len(self.train_data)\n",
        "        else:\n",
        "            return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> dict:\n",
        "        if self.mode == \"train\":\n",
        "            row = self.train_data.iloc[idx]\n",
        "        else:\n",
        "            row = self.data.iloc[idx]\n",
        "\n",
        "        target_img_name = os.path.join(self.path, self.images_folder, row[\"target\"])\n",
        "        target_image = Image.open(target_img_name).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            target_image = self.transform(target_image)\n",
        "\n",
        "        candidate_images = []\n",
        "        for i in range(1, 10):\n",
        "            img_name = os.path.join(self.path, self.images_folder, row[f\"image_{i}\"])\n",
        "            image = Image.open(img_name).convert(\"RGB\")\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            candidate_images.append(image)\n",
        "        candidate_images = torch.stack(candidate_images)\n",
        "\n",
        "        sample = {\n",
        "            \"word\": row[\"word\"],\n",
        "            \"context\": row[\"context\"],\n",
        "            \"target\": torch.Tensor(target_image),\n",
        "            \"candidate_images\": candidate_images,\n",
        "        }\n",
        "        return sample\n"
      ],
      "metadata": {
        "id": "BdzzftzxoyBe"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from typing import Literal\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import (\n",
        "    CenterCrop,\n",
        "    Compose,\n",
        "    InterpolationMode,\n",
        "    Normalize,\n",
        "    Resize,\n",
        "    ToTensor,\n",
        ")\n",
        "\n",
        "\n",
        "transform = Compose(\n",
        "    [\n",
        "        Resize(224, interpolation=InterpolationMode.BICUBIC),\n",
        "        CenterCrop(224),\n",
        "        ToTensor(),\n",
        "        # Normalize(\n",
        "        #     (0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)\n",
        "        # ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def get_loaders(\n",
        "    path: str,\n",
        "    csv_file: str,\n",
        "    images_folder: str,\n",
        "    transform: Compose = transform,\n",
        "    mode: Literal[\"train\", \"eval\"] = \"eval\",\n",
        "    batch_size: int = 1,\n",
        "    num_workers: int = 0,\n",
        "    shuffle: bool = True,\n",
        "    split_ratio: float = 0.8,\n",
        ") -> DataLoader | tuple[DataLoader, DataLoader]:\n",
        "    if mode == \"eval\":\n",
        "        eval_dataset = VisualWSDDataset(\n",
        "            path=path,\n",
        "            csv_file=csv_file,\n",
        "            images_folder=images_folder,\n",
        "            transform=transform,\n",
        "            mode=\"eval\",\n",
        "        )\n",
        "        eval_loader = DataLoader(\n",
        "            eval_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers\n",
        "        )\n",
        "        return eval_loader\n",
        "\n",
        "    elif mode == \"train\":\n",
        "        train_dataset = VisualWSDDataset(\n",
        "            path=path,\n",
        "            csv_file=csv_file,\n",
        "            images_folder=images_folder,\n",
        "            transform=transform,\n",
        "            mode=\"train\",\n",
        "            split_ratio=split_ratio,\n",
        "        )\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle,\n",
        "            num_workers=num_workers,\n",
        "        )\n",
        "\n",
        "        test_dataset = VisualWSDDataset(\n",
        "            path=path,\n",
        "            csv_file=csv_file,\n",
        "            images_folder=images_folder,\n",
        "            transform=transform,\n",
        "            mode=\"train\",\n",
        "            split_ratio=split_ratio,\n",
        "            test_split=True,\n",
        "        )\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers\n",
        "        )\n",
        "\n",
        "        return train_loader, test_loader\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f\"Invalid mode. Choose 'train' or 'eval'. Provided mode: {mode}\"\n",
        "        )\n",
        "\n",
        "\n",
        "def get_metrics(targets: list, ranks: list) -> tuple[float]:\n",
        "    accuracy = sum(targets) / len(targets)\n",
        "\n",
        "    f1 = f1_score(targets, [1] * len(targets))\n",
        "\n",
        "    mrr = np.mean([1 / rank for rank in ranks])\n",
        "\n",
        "    return accuracy, f1, mrr\n",
        "\n",
        "\n",
        "def seed_everything(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n"
      ],
      "metadata": {
        "id": "iOlPDODHmHiT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class BaseModel(ABC, nn.Module):\n",
        "    \"\"\"\n",
        "    An abstract base class for models for Visual-WSD dataset.\n",
        "\n",
        "    Attributes:\n",
        "        image_processor: A nn.Module or similar object responsible for processing images.\n",
        "        text_processor: A nn.Module or similar object responsible for processing text.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.image_processor = None\n",
        "        self.text_processor = None\n",
        "\n",
        "    @abstractmethod\n",
        "    def process_image(self, images: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Process the images.\n",
        "\n",
        "        Args:\n",
        "            images (torch.Tensor): A tensor containing the one image or stacked multiple images.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The processed images.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def process_text(self, phrase: str, word: str) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Process the textual input.\n",
        "\n",
        "        Args:\n",
        "            phrase (str): The phrase containing the ambiguous word.\n",
        "            word (str): The ambiguous word itself.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The processed text.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, data: dict[str, torch.Tensor]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        The forward pass of the model. Should handle both text and image data, and return a tensor of logits,\n",
        "        where on first place would be logit for target.\n",
        "\n",
        "        Args:\n",
        "            data (Dict[str, torch.Tensor]): A dictionary containing textual and visual inputs.\n",
        "            Expected keys are 'word', 'context', 'target', and 'other_images'.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor of logits of size [batch_size, 10].\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "\n",
        "class ClipModel(BaseModel):\n",
        "    \"\"\"\n",
        "    https://huggingface.co/docs/transformers/model_doc/clip\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name):\n",
        "        super().__init__()\n",
        "        self.model = CLIPModel.from_pretrained(model_name)\n",
        "        self.processor = CLIPProcessor.from_pretrained(model_name, do_rescale=False)\n",
        "\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "\n",
        "    def process_image(self, images: torch.Tensor) -> torch.Tensor:\n",
        "        processed_images = self.processor(images=images, return_tensors=\"pt\", dim=2).to(\n",
        "            self.device\n",
        "        )\n",
        "        return processed_images\n",
        "\n",
        "\n",
        "    def process_text(self, texts: list) -> torch.Tensor:\n",
        "        processed_texts = self.processor(text=texts, return_tensors=\"pt\", padding=True).to(\n",
        "            self.device\n",
        "        )\n",
        "        return processed_texts\n",
        "\n",
        "\n",
        "    def forward(self, images: torch.Tensor, texts: list):\n",
        "        images = images.to(self.device)\n",
        "        logits = torch.zeros(images.shape[0], images.shape[1])\n",
        "\n",
        "        for idx, sample_images in enumerate(images):\n",
        "            processed_sample_images = self.process_image(sample_images)\n",
        "            processed_phrase = self.process_text(texts[idx])\n",
        "\n",
        "            output = self.model(\n",
        "                input_ids=processed_phrase.input_ids,\n",
        "                pixel_values=processed_sample_images.pixel_values,\n",
        "                return_dict=True,\n",
        "            )\n",
        "            logits[idx] = output.logits_per_image.squeeze(1)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "## EVERYTHING BELOW IS CHECK\n",
        "# image_urls = [\n",
        "#     \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n",
        "# ]\n",
        "# texts = [\"a photo of a cat\", \"a photo of a dog\"]\n",
        "\n",
        "\n",
        "# def load_image(url):\n",
        "#     response = requests.get(url)\n",
        "#     img = Image.open(BytesIO(response.content))#.convert(\"RGB\")\n",
        "#     transform = transforms.ToTensor()\n",
        "#     return transform(img)\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     model_name = \"openai/clip-vit-base-patch32\"\n",
        "#     model = ClipModel(model_name=model_name)\n",
        "\n",
        "#     images = torch.stack([load_image(url) for url in image_urls])\n",
        "#     output = model(images, texts)\n",
        "#     print(output)\n"
      ],
      "metadata": {
        "id": "Cydiz9QHrHXD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def evaluate_model(\n",
        "    model: torch.nn.Module, data_loader: DataLoader\n",
        ") -> dict[str, float | list]:\n",
        "    model.eval()\n",
        "\n",
        "    predicted_images = []  # store which image was predicted\n",
        "    correct_preds = []  # store whether the target was correctly predicted (1) or (0)\n",
        "    all_target_ranks = []  # store the rank of the target in each prediction\n",
        "    phrases = []  # store input phrases for further analysis\n",
        "    all_probs = []  # store the probabilities for further analysis\n",
        "\n",
        "    loop = tqdm(enumerate(data_loader), total=len(data_loader))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in loop:\n",
        "            phrases.extend(list(batch[\"context\"]))\n",
        "            texts = batch['context']\n",
        "\n",
        "            target, candidate_images = batch['target'], batch['candidate_images']\n",
        "            images = torch.cat([target.unsqueeze(1), candidate_images], dim=1)\n",
        "\n",
        "            logits = model(images, texts)\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "\n",
        "            top_prob, top_indices = torch.max(probs, dim=1)\n",
        "            predicted_images.extend([pred.item() for pred in top_indices])\n",
        "\n",
        "            for i in range(len(top_indices)):\n",
        "                correct_target = 1 if top_indices[i] == 0 else 0\n",
        "                correct_preds.append(correct_target)\n",
        "\n",
        "                rank = (probs[i].sort(descending=True)[1] == 0).nonzero(as_tuple=True)[\n",
        "                    0\n",
        "                ].item() + 1\n",
        "                all_target_ranks.append(rank)\n",
        "\n",
        "                all_probs.append(probs[i].tolist())\n",
        "\n",
        "    accuracy, f1, mrr = get_metrics(correct_preds, all_target_ranks)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": f1,\n",
        "        \"mrr\": mrr,\n",
        "        \"phrases\": phrases,\n",
        "        \"predictions\": predicted_images,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "NBCBE0gT9wgn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = get_loaders(\n",
        "    path = 'data/visual_wsd',\n",
        "    csv_file = 'dataset.csv',\n",
        "    images_folder = 'images',\n",
        "    transform = transform,\n",
        "    mode = \"eval\",\n",
        "    batch_size = 32,\n",
        "    num_workers = 2\n",
        ")"
      ],
      "metadata": {
        "id": "1op7wMypou48"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"openai/clip-vit-base-patch32\"\n",
        "model = ClipModel(model_name=model_name)"
      ],
      "metadata": {
        "id": "8QXQ77gk-4h0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = evaluate_model(model, loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXJt-_b7-u5a",
        "outputId": "a105199d-d03f-4878-be17-9615959ea43d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|‚ñç         | 18/403 [07:18<2:30:15, 23.42s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "id": "GyxgumTU_Hwq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed98e94b-7f05-4896-d734-cf98da7a379a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.59375,\n",
              " 'f1': 0.7450980392156863,\n",
              " 'mrr': 0.7492559523809523,\n",
              " 'phrases': ['moorhen swamphen',\n",
              "  'serinus genus',\n",
              "  'pegmatite igneous',\n",
              "  'bangalores torpedo',\n",
              "  'bonxie skua',\n",
              "  'ixia genus',\n",
              "  'leucaena genus',\n",
              "  'mahonia genus',\n",
              "  'attalea genus',\n",
              "  'fagaceae family',\n",
              "  'gangster outlaw',\n",
              "  'upset success',\n",
              "  'brevicipitidae family',\n",
              "  'iridium metal',\n",
              "  'breakdown failure',\n",
              "  'catharanthus genus',\n",
              "  'leucanthemum genus',\n",
              "  'biro pen',\n",
              "  'maja genus',\n",
              "  'boletellus genus',\n",
              "  'beater implement',\n",
              "  'capparis genus',\n",
              "  'serenoa genus',\n",
              "  'sticherus genus',\n",
              "  'entoloma genus',\n",
              "  'foulard fabric',\n",
              "  'snert soup',\n",
              "  'biryani dish',\n",
              "  'sobriquet appellation',\n",
              "  'pigiron iron',\n",
              "  'menhaden clupeid',\n",
              "  'sprat sardine'],\n",
              " 'predictions': [0,\n",
              "  3,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  3,\n",
              "  3,\n",
              "  0,\n",
              "  5,\n",
              "  4,\n",
              "  0,\n",
              "  5,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  3,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  4,\n",
              "  3,\n",
              "  0,\n",
              "  0,\n",
              "  3,\n",
              "  6,\n",
              "  5,\n",
              "  7,\n",
              "  0,\n",
              "  0]}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}